# ğŸ¤– Playwright Data Pipeline â€” Automated Web Data Collection & Processing

> End-to-end automated data collection system: scrape â†’ validate â†’ clean â†’ store â†’ report.

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat&logo=python&logoColor=white)
![Playwright](https://img.shields.io/badge/Playwright-2EAD33?style=flat&logo=playwright&logoColor=white)
![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=flat&logo=javascript&logoColor=black)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=flat&logo=pandas)
![MySQL](https://img.shields.io/badge/MySQL-00000F?style=flat&logo=mysql&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=flat)

---

## ğŸ§  What This Project Does

This pipeline automates the full lifecycle of web-sourced data: from browser-based collection to structured, analytics-ready output. It replaces manual copy-paste workflows with a reliable, scheduled, error-handled automation system.

**Pipeline stages:**
1. **Collect** â€” Playwright browser automation extracts structured data from target web sources
2. **Validate** â€” Schema checks, type enforcement, and completeness validation on raw data
3. **Clean** â€” Pandas-based deduplication, normalisation, and null handling
4. **Store** â€” Structured output to MySQL database or CSV
5. **Report** â€” Auto-generated summary report with Plotly visualisations

---

## ğŸ—‚ Project Structure

```
playwright-data-pipeline/
â”œâ”€â”€ collector/
â”‚   â”œâ”€â”€ scraper.py          # Playwright page interaction logic
â”‚   â”œâ”€â”€ scheduler.py        # Cron-style job scheduling
â”‚   â””â”€â”€ config.py           # Target URLs and selectors (configurable)
â”œâ”€â”€ processor/
â”‚   â”œâ”€â”€ validator.py        # Schema and completeness checks
â”‚   â”œâ”€â”€ cleaner.py          # Pandas cleaning pipeline
â”‚   â””â”€â”€ transformer.py      # Type casting, normalisation, enrichment
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ db.py               # MySQL connection and insert logic
â”‚   â””â”€â”€ exporter.py         # CSV / JSON export utilities
â”œâ”€â”€ reporter/
â”‚   â””â”€â”€ report.py           # Plotly summary visualisations
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py    # Unit tests for each stage
â”œâ”€â”€ main.py                 # Run the full pipeline
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš¡ Quick Start

```bash
git clone https://github.com/dhanasekaranmariappan/playwright-data-pipeline.git
cd playwright-data-pipeline
pip install -r requirements.txt
playwright install chromium
```

```python
# Run the full pipeline
python main.py

# Or run individual stages
from collector.scraper import collect
from processor.cleaner import clean
from storage.db import save

raw = collect(url="https://example.com/data-table")
clean_df = clean(raw)
save(clean_df, table="collected_data")
```

---

## ğŸ” Pipeline Flow

```
[Target Website]
      â”‚
      â–¼
[Playwright Scraper] â”€â”€â–º Raw Data (JSON/Dict)
      â”‚
      â–¼
[Validator] â”€â”€â–º Flag missing/malformed records
      â”‚
      â–¼
[Pandas Cleaner] â”€â”€â–º Deduplicated, typed, normalised DataFrame
      â”‚
      â”œâ”€â”€â–º [MySQL Storage] â”€â”€â–º Queryable database table
      â”‚
      â””â”€â”€â–º [Plotly Reporter] â”€â”€â–º HTML summary dashboard
```

---

## âœ… Features

- **Headless browser automation** â€” handles JavaScript-rendered pages
- **Retry logic** â€” automatically retries failed page loads with exponential backoff
- **Scheduling** â€” configurable run intervals (hourly, daily, custom cron)
- **Validation layer** â€” catches bad data before it enters your database
- **Modular stages** â€” run any stage independently or chain them all
- **Configurable selectors** â€” swap target sites without rewriting core logic

---

## ğŸ›  Tech Stack

| Tool | Purpose |
|------|---------|
| Playwright (Python) | Browser automation & data extraction |
| JavaScript | Page interaction scripts where needed |
| Pandas | Data cleaning and transformation |
| MySQL | Structured data storage |
| Plotly | Output visualisation and reporting |
| Schedule | Lightweight job scheduling |

---

## ğŸ”— Related Projects

- [eda-toolkit](../eda-toolkit) â€” EDA library used to analyse datasets collected by this pipeline
- [ml-learning-lab](../ml-learning-lab) â€” ML experiments using this pipeline's outputs as training data

---

## ğŸ‘¤ Author

**Dhanasekaran Mariappan** â€” Python Data Engineer | Learning ML Engineering  
ğŸ“§ dhanasekaranmariappan2202@gmail.com | [LinkedIn](https://www.linkedin.com/in/dhanasekaran-mariappan-22feb2000/) | Open to EU relocation
